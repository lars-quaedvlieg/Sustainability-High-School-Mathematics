
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>The Reinforcement Learning Problem</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://kit.fontawesome.com/26eda3309b.js"></script>
    <link rel="canonical" href="https://lars-quaedvlieg.github.io/RL-Playground/content/intro-to-rl/the-rl-problem.html" />
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Markov Decision Processes" href="markov-decision-processes.html" />
    <link rel="prev" title="Credits" href="../credits.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../welcome.html">
   The Reinforcement Learning Playground
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../credits.html">
   Credits
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Fundamentals of Reinforcement Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   The Reinforcement Learning Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="markov-decision-processes.html">
   Markov Decision Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="planning-dynamic-programming.html">
   Planning by Dynamic Programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model-free-prediction.html">
   Model-Free Prediction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model-free-control.html">
   Model-Free Control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="integrating-learning-planning.html">
   Integrating Learning and Planning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exploration-vs-exploitation.html">
   Exploration and Exploitation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Value-Based Deep Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../deep-rl/value-function-approximation.html">
   Value Function Approximation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../deep-rl/dqn.html">
   Deep Q-Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../deep-rl/ddqn.html">
   Double Deep Q-Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Policy Gradient Algorithms
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../pg-algorithms/pg-methods.html">
   Policy Gradient Methods
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Multiagent Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../marl/rl-classic-games.html">
   Reinforcement Learning in Classic Games
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Project for "HUM-433 How People Learn II" at <a target="_blank" href="https://edu.epfl.ch/coursebook/en/how-people-learn-designing-learning-tools-ii-HUM-433">École polytechnique fédérale de Lausanne Powered</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/content/intro-to-rl/the-rl-problem.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   The Reinforcement Learning Problem
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#components-of-a-reinforcement-learning-agent">
   Components of a Reinforcement Learning Agent
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>The Reinforcement Learning Problem</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   The Reinforcement Learning Problem
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#components-of-a-reinforcement-learning-agent">
   Components of a Reinforcement Learning Agent
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="the-reinforcement-learning-problem">
<h1>The Reinforcement Learning Problem<a class="headerlink" href="#the-reinforcement-learning-problem" title="Permalink to this headline">¶</a></h1>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Please note that this notebook is still <strong>work in progress</strong>, hence some sections could behave unexpectedly, or provide
incorrect information.</p>
</div>
<p>Reinforcement Learning (RL) is an area of Machine Learning concerned with deciding on a sequence of actions in an
unknown environment in order to maximize cumulative reward.</p>
<p>To give an idea of this, imagine you are somewhere in a 2d-maze. At each point, you can either move to the left, right, up or down. The goal is to find your way out of the maze.
This corresponds to obtaining positive reward when completing the maze. Using Reinforcement Learning, you can figure out the optimal way to behave in this environment.</p>
<div class="section" id="id1">
<h2>The Reinforcement Learning Problem<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>What makes reinforcement learning different from other machine learning paradigms?</p>
<ul class="simple">
<li><p>There is no supervisor, only a <strong>reward</strong> signal</p></li>
<li><p>Feedback is delayed, not instantaneous</p></li>
<li><p>Time really matters (sequential, non i.i.d. data)</p></li>
<li><p>Agent’s actions affect the subsequent data it receives</p></li>
</ul>
<div class="proof definition admonition" id="definiton:reward-hypothesis">
<p class="admonition-title"><span class="caption-number">Definition 1 </span></p>
<div class="definition-content section" id="proof-content">
<p>The <em>Reward Hypothesis</em> states that all imaginable goals can be described by the maximization of an expected cumulative
reward function.</p>
</div>
</div><p>Rewards function as scalar feedback signals. Reinforcement Learning is based on the <strong>Reward Hypothesis</strong>, which has
been assumed to be true. Some problems can be difficult to solve, since actions can have long-term consequences and
reward can be delayed.</p>
<div class="proof example dropdown admonition" id="example:chess-reward-function">
<p class="admonition-title"><span class="caption-number">Example 1 </span></p>
<div class="example-content section" id="proof-content">
<p>Imagine our goal is to train the best computer program at chess. What could a good reward function be defined as?</p>
<details class="sphinx-bs dropdown card mb-3">
<summary class="summary-title card-header">
Reveal answer<div class="summary-down docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="summary-up docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="summary-content card-body docutils">
<p class="card-text">There could be multiple correct answers (where some definitions of the reward function may lead to solving the problem
more quickly than others). However, one example is giving a positive reward whenever the program wins a game. It will
aim to maximize the cumulative reward, so over time it should get better at the game.</p>
</div>
</details></div>
</div><div class="proof example dropdown admonition" id="example:non-trivial-reward-function">
<p class="admonition-title"><span class="caption-number">Example 2 </span></p>
<div class="example-content section" id="proof-content">
<p>Imagine you have built an agent that controls the way electricity is shipped to houses from a provider. Your goal is to
save as much money as you can, by modifying the shipment procedure. As an AI engineer, you decide to define the reward
function as punishing shipped electricity, hoping the agent would make the procedure more efficient by not shipping
unwanted electricity.</p>
<ul class="simple">
<li><p>Why may the design of this reward function not be the best idea?</p></li>
</ul>
<details class="sphinx-bs dropdown card mb-3">
<summary class="summary-title card-header">
Reveal answer<div class="summary-down docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="summary-up docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="summary-content card-body docutils">
<p class="card-text">By punishing shipped electricity, there is a good chance that the agent will learn not to ship anything at all.
This would maximize it’s cumulative reward.</p>
</div>
</details><ul class="simple">
<li><p>How would you improve the reward function?</p></li>
</ul>
<details class="sphinx-bs dropdown card mb-3">
<summary class="summary-title card-header">
Reveal answer<div class="summary-down docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="summary-up docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="summary-content card-body docutils">
<p class="card-text">There can be multiple correct answers, but one might define a reward function directly as the profit that has been
earned by the procedure. This should then be maximize to the best of the agent’s abilities.</p>
<p class="card-text">This was just a toy example, but the point here is that reward functions may have undesirable outcomes (that could, in
some cases, have bad consequences). The definition of a good reward function may not be trivial in some situations.</p>
</div>
</details></div>
</div><div class="proof definition admonition" id="definiton:markov-property">
<p class="admonition-title"><span class="caption-number">Definition 2 </span></p>
<div class="definition-content section" id="proof-content">
<p>A stochastic process <span class="math notranslate nohighlight">\(\{X_t\}_{t \in T}\)</span>, with values <span class="math notranslate nohighlight">\(X_0, X_1, \dotsc, X_T\)</span> from <span class="math notranslate nohighlight">\(t = 0\)</span> until <span class="math notranslate nohighlight">\(t = T\)</span> is said to have
the <em>Markov Property</em>, if and only if</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(X_{t+1} | X_t) = \mathbb{P}(X_{t+1} | X_0, ..., X_t)
\]</div>
</div>
</div><p>Let <span class="math notranslate nohighlight">\(S_t^a\)</span> and <span class="math notranslate nohighlight">\(S_t^e\)</span> be the state of the agent and the environment at any time <span class="math notranslate nohighlight">\(t\)</span>. If the environment is
<strong>fully-observable</strong>, then <span class="math notranslate nohighlight">\(S_t^a = S_t^e\)</span>. Reinforcement learning environments can be seen as stochastic processes
<span class="math notranslate nohighlight">\(\{S_t\}_{t \in T}\)</span>.</p>
<p>For an environment, having the Markov property means that, for all possible next states, the probability of obtaining
that future state (<span class="math notranslate nohighlight">\(S_{t+1}\)</span>) solely depends on the current state (<span class="math notranslate nohighlight">\(S_t\)</span>), and not on any previous states
(<span class="math notranslate nohighlight">\(S_{t-1}, \dotsc, S_0\)</span>). This property describes that the current state is a sufficient statistic of the future, and
history does not matter.</p>
<p>This will prove to be a key property in Reinforcement Learning, and it will be useful in many scenarios in the book.
But what happens when the markov property does not hold?</p>
<p>The class of <strong>partially-observable</strong> environments do not have the Markov property. Here, the agent indirectly observes
the environment, meaning it may not have all information that is needed to know what happens next. Now,
<span class="math notranslate nohighlight">\(S_t^a \neq S_t^e\)</span>. Since history is now important to predict the future, the agent must construct its own state
representation <span class="math notranslate nohighlight">\(S_t^a\)</span>. For example:</p>
<ul class="simple">
<li><p>Complete history: <span class="math notranslate nohighlight">\(S_t^a = H_t = (O_0, O_1, \dotsc, O_t)\)</span></p></li>
<li><p>Beliefs of environment state: <span class="math notranslate nohighlight">\(S_t^a = (\mathbb{P}[S_t^e = s^{(1)}], \dotsc,\mathbb{P}[S_t^e = s^{(n)}])\)</span></p></li>
<li><p>Recurrent Neural Network: <span class="math notranslate nohighlight">\(S_t^a = \sigma(S_{t-1}^a W_s + O_t W_o))\)</span></p></li>
</ul>
<p>Here, <span class="math notranslate nohighlight">\(O_t\)</span> is the observed state at time <span class="math notranslate nohighlight">\(t\)</span>. These problems are typically much more difficult to solve, since the
Markov property cannot be exploited.</p>
<div class="proof example dropdown admonition" id="example:vase-non-markovian">
<p class="admonition-title"><span class="caption-number">Example 3 </span></p>
<div class="example-content section" id="proof-content">
<p>Imagine we have the following process. There exists a dark vase with three balls. One is red, one is green, and one is
blue. Every <span class="math notranslate nohighlight">\(t\)</span>, we take a ball out <strong>without replacement</strong>. Does this environment have the Markov property? Why or why
not?</p>
<details class="sphinx-bs dropdown card mb-3">
<summary class="summary-title card-header">
Reveal answer<div class="summary-down docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="summary-up docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="summary-content card-body docutils">
<p class="card-text">This is a non-Markovian stochastic process. Recall that the markov property comes down to</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(X_{t+1} | X_t) = \mathbb{P}(X_{t+1} | X_0, ..., X_t)
\]</div>
<p class="card-text">Here, <span class="math notranslate nohighlight">\(X_t\)</span> is the colour of the ball at time <span class="math notranslate nohighlight">\(t\)</span>. Imagine we have the order <span class="math notranslate nohighlight">\(X_0 = \)</span> red, <span class="math notranslate nohighlight">\(X_1 = \)</span> green, and <span class="math notranslate nohighlight">\(X_2 = \)</span>
blue.</p>
<p class="card-text">Let’s first think about <span class="math notranslate nohighlight">\(\mathbb{P}(X_2 | X_1)\)</span>. If we only know that <span class="math notranslate nohighlight">\(X_1\)</span> is green, what do we know about <span class="math notranslate nohighlight">\(X_{2}\)</span>?
We know the <span class="math notranslate nohighlight">\(X_2\)</span> must be <em>either</em> blue or green.</p>
<p class="card-text">However, <span class="math notranslate nohighlight">\(\mathbb{P}(X_{t+1} | X_0, ..., X_t)\)</span> means we know that both <span class="math notranslate nohighlight">\(X_0\)</span> is red and <span class="math notranslate nohighlight">\(X_1\)</span> is green. Hence, <span class="math notranslate nohighlight">\(X_2\)</span>
<em>must</em> be blue.</p>
<p class="card-text">We now see that here <span class="math notranslate nohighlight">\(\mathbb{P}(X_{t+1} | X_t) \neq \mathbb{P}(X_{t+1} | X_0, ..., X_t)\)</span>. Hence, the Markov property
does not hold for this stochastic process.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p class="card-text">This type of environment would actually be partially-observable, since we agent cannot directly observe the underlying
state. Instead, we must maintain a probability distribution of different observations given the underlying state.</p>
</div>
</div>
</details></div>
</div><div class="proof example dropdown admonition" id="example:poker-partially-observable">
<p class="admonition-title"><span class="caption-number">Example 4 </span></p>
<div class="example-content section" id="proof-content">
<p>Is the game of Poker a fully- or partially-observable environment?</p>
<details class="sphinx-bs dropdown card mb-3">
<summary class="summary-title card-header">
Reveal answer<div class="summary-down docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="summary-up docutils">
<svg version="1.1" width="24" height="24" class="octicon octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="summary-content card-body docutils">
<p class="card-text">Poker is partially-observable, since each player can only see their own cards. This means that we don’t really know
which state of the game we are in. Full observability would require knowledge about every player’s cards.</p>
<p class="card-text">A large part of the game is trying to estimate the quality of other players’ cards. You can see partial observability
makes the game a lot more difficult to play. A good agent would thus need to create good estimates of the environment
states (for example by predicting other players’ card qualities) to perform well.</p>
</div>
</details></div>
</div></div>
<div class="section" id="components-of-a-reinforcement-learning-agent">
<h2>Components of a Reinforcement Learning Agent<a class="headerlink" href="#components-of-a-reinforcement-learning-agent" title="Permalink to this headline">¶</a></h2>
<p>An RL agent may include one or more of these components:</p>
<ul class="simple">
<li><p><strong>Policy</strong>: agent’s behaviour function</p></li>
<li><p><strong>Value function</strong>: how good is each state and/or action</p></li>
<li><p><strong>Model</strong>: agent’s representation of the environment</p></li>
</ul>
<p>A policy describes the agent’s behavior. It maps states to actions. You can have deterministic (<span class="math notranslate nohighlight">\(a = \pi(s)\)</span>) and
stochastic policies (<span class="math notranslate nohighlight">\(\pi(a | s) = \mathbb{P}(A_t = a | S_t = s)\)</span>). Often, <span class="math notranslate nohighlight">\(\pi\)</span> is used to denote a policy.</p>
<p>A <strong>value function</strong> is a prediction of future reward of a given state. You can use it to determine if a state is good
or bad. This means you can use it to select actions. It can be computed by <span class="math notranslate nohighlight">\(v_\pi(s) = \mathbb{E}_\pi(G_t | S_t = s)\)</span>,
where <span class="math notranslate nohighlight">\(G_t\)</span> is the <strong>return</strong> (or discounted cumulative reward). The return is defined as
<span class="math notranslate nohighlight">\(G_t = R_1 + \gamma R_2 + \gamma^2 R_3 + ... = \sum_{i=t+1}^\infty\gamma^{i-t-1}R_i\)</span> for some <span class="math notranslate nohighlight">\(\gamma \in [0, 1]\)</span>.
This gamma is the <strong>discount factor</strong>, and it influences how much the future impacts return. This is useful, since
it is not known if the representation of the environment is perfect. If it is not, it is not good to let the future
influence the return as much as more local states. So, it is discounted.</p>
<p>Finally, a <strong>model</strong> predicts what the environment will do next. We let
<span class="math notranslate nohighlight">\(P_{ss'}^a = \mathbb{P}(S_{t+1} = s' | S_t = s, A_t = a)\)</span> and <span class="math notranslate nohighlight">\(R_{s}^a = \mathbb{P}(R_{t+1} | S_t = s, A_t = a)\)</span>.
<span class="math notranslate nohighlight">\(P\)</span> (<strong>Transition model</strong>) is the probability of transitioning to a next state given an action, while R is the
reward when taking an action in some state.</p>
<table class="table" id="table-rl-agent-categories">
<caption><span class="caption-number">Table 2 </span><span class="caption-text">Types of Reinforcement Learning agents</span><a class="headerlink" href="#table-rl-agent-categories" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Category</p></th>
<th class="head"><p>Properties</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Value based</p></td>
<td><p>No Policy (implicit), Value function</p></td>
</tr>
<tr class="row-odd"><td><p>Policy based</p></td>
<td><p>Policy, No Value function</p></td>
</tr>
<tr class="row-even"><td><p>Actor Critic</p></td>
<td><p>Policy, Value function</p></td>
</tr>
<tr class="row-odd"><td><p>Model Free</p></td>
<td><p>No Model of the environment</p></td>
</tr>
<tr class="row-even"><td><p>Model based</p></td>
<td><p>Model of the environment</p></td>
</tr>
</tbody>
</table>
<p>RL Agents can be categorized into the categories that are listed in <a class="reference internal" href="#table-rl-agent-categories"><span class="std std-numref">Table 2</span></a>. These can require
different approaches that will be discussed throughout the book.</p>
<p>There are two fundamental problems in <strong>sequential decision making</strong>.</p>
<ul class="simple">
<li><p>Reinforcement Learning</p>
<ul>
<li><p>The environment is initially unknown</p></li>
<li><p>The agent interacts with the environment</p></li>
<li><p>The agent improves its policy</p></li>
</ul>
</li>
<li><p>Planning (e.g. deliberation, reasoning, introspection, pondering, thought, search)</p>
<ul>
<li><p>A model of the environment is known</p></li>
<li><p>The agent performs computations with its model (without any external interaction)</p></li>
<li><p>The agent improves its policy</p></li>
</ul>
</li>
</ul>
<p>It is important for an agent to make a trade-off between exploration and exploitation as well. Depending on the choice
in this trade-off, agents will be more or less flexible and may or may not find better actions to perform.</p>
<ul class="simple">
<li><p><strong>Exploration</strong> finds more information about the environment</p></li>
<li><p><strong>Exploitation</strong> exploits known information to maximize reward</p></li>
</ul>
<p>Finally, it is possible to differentiate between prediction and control. <strong>Prediction</strong> is about evaluating the future
given a certain policy, while <strong>control</strong> is about finding the best policy to optimize the future.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content\intro-to-rl"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="../credits.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Credits</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="markov-decision-processes.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Markov Decision Processes</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Lars Quaedvlieg<br/>
    
        &copy; Copyright 2023.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>